{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d25e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "import torch, gc\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "from src.data_utils import clean_string, split_x_target_by_words\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c8c3d46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x114e8acf0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d807e9",
   "metadata": {},
   "source": [
    "## Get the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ea332c",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b04f557",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/tweets_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b925d2e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train texts: 1280000, Val_Test texts: 320000\n",
      "Val texts: 160000, Test texts: 160000\n"
     ]
    }
   ],
   "source": [
    "val_test_size = 0.20\n",
    "test_size = 0.50\n",
    "\n",
    "train_texts, val_test_texts = train_test_split(list(df[\"cleaned_text\"]), test_size=val_test_size, random_state=42)\n",
    "print(f\"Train texts: {len(train_texts)}, Val_Test texts: {len(val_test_texts)}\")\n",
    "val_texts, test_texts = train_test_split(val_test_texts, test_size=test_size, random_state=42)\n",
    "print(f\"Val texts: {len(val_texts)}, Test texts: {len(test_texts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f206ea",
   "metadata": {},
   "source": [
    "### Pre-trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a95ff719",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_eval_pairs(texts, max_examples: int = 2000):\n",
    "    \"\"\"Build validation x/targets from splited df parts\"\"\"\n",
    "    pairs = []\n",
    "    for t in texts:\n",
    "        t = clean_string(t)\n",
    "        if not t:\n",
    "            continue\n",
    "        x, target = split_x_target_by_words(t)\n",
    "        if x and target:\n",
    "            pairs.append((x, target))\n",
    "        if len(pairs) >= max_examples:\n",
    "            break\n",
    "    X = [p for p,_ in pairs]\n",
    "    targets = [r for _,r in pairs]\n",
    "    return X, targets\n",
    "\n",
    "val_prefixes, val_refs = build_eval_pairs(test_texts, max_examples=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d7f643d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available()\n",
    "          else \"mps\" if torch.backends.mps.is_available()\n",
    "          else \"cpu\")\n",
    "\n",
    "model_name = \"distilbert/distilgpt2\"\n",
    "\n",
    "gpt_tk = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\")\n",
    "\n",
    "if gpt_tk.pad_token_id is None:\n",
    "    gpt_tk.pad_token = gpt_tk.eos_token\n",
    "\n",
    "gpt_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=(torch.float16 if device.type in {\"cuda\",\"mps\"} else None),\n",
    "    low_cpu_mem_usage=True\n",
    "    ).to(device).eval()\n",
    "\n",
    "gpt_model.config.pad_token_id = gpt_tk.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41b5108",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def complete_with_distilgpt2_batched(X, *, max_new_tokens=64, do_sample=True,\n",
    "                                     top_k=50, top_p=0.95, temperature=0.8,\n",
    "                                     repetition_penalty=1.1):\n",
    "    enc = gpt_tk(X, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    input_ids = enc[\"input_ids\"].to(device)\n",
    "    attention_mask = enc[\"attention_mask\"].to(device)\n",
    "    gen_cfg = GenerationConfig(\n",
    "        max_new_tokens=max_new_tokens, do_sample=do_sample,\n",
    "        top_k=top_k, top_p=top_p, temperature=temperature,\n",
    "        repetition_penalty=repetition_penalty,\n",
    "        eos_token_id=gpt_tk.eos_token_id, pad_token_id=gpt_tk.pad_token_id,\n",
    "        use_cache=True  # default, keep for speed\n",
    "    )\n",
    "    out = gpt_model.generate(input_ids=input_ids,\n",
    "                             attention_mask=attention_mask,\n",
    "                             generation_config=gen_cfg)\n",
    "    \n",
    "    # slice tail per example using true X lengths\n",
    "    lens = attention_mask.sum(dim=1)\n",
    "    gens = []\n",
    "    for i in range(out.size(0)):\n",
    "        start = int(lens[i].item())\n",
    "        gen_ids = out[i, start:]\n",
    "        gens.append(gpt_tk.decode(gen_ids, skip_special_tokens=True))\n",
    "    return gens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b016c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = rouge_scorer.RougeScorer([\"rouge1\",\"rouge2\"], use_stemmer=False)\n",
    "\n",
    "def eval_rouge_model_batched(X, targets, gen_params, bs=32, max_new_tokens=64):\n",
    "    r1 = r2 = n = 0\n",
    "    for s in range(0, len(X), bs):\n",
    "        batch_p = X[s:s+bs]\n",
    "        batch_r = targets[s:s+bs]\n",
    "        preds = complete_with_distilgpt2_batched(batch_p, max_new_tokens=max_new_tokens, **gen_params)\n",
    "        for p, g in zip(preds, batch_r):\n",
    "            s_ = scorer.score(g, p)\n",
    "            r1 += s_[\"rouge1\"].fmeasure\n",
    "            r2 += s_[\"rouge2\"].fmeasure\n",
    "            n  += 1\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    return {\"rouge1\": r1/max(n,1), \"rouge2\": r2/max(n,1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0675b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_grid = [\n",
    "               dict(do_sample=True, top_k=50, top_p=0.95, temperature=0.8),\n",
    "               dict(do_sample=True, top_k=0, top_p=0.90, temperature=0.7), # nucleus only \n",
    "               dict(do_sample=True, top_k=100, top_p=0.95, temperature=0.9, repetition_penalty=1.1),\n",
    "               ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6685f2a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CFG: {'do_sample': True, 'top_k': 50, 'top_p': 0.95, 'temperature': 0.8} -> ROUGE1=0.028 ROUGE2=0.001\n",
      "CFG: {'do_sample': True, 'top_k': 0, 'top_p': 0.9, 'temperature': 0.7} -> ROUGE1=0.029 ROUGE2=0.001\n",
      "CFG: {'do_sample': True, 'top_k': 100, 'top_p': 0.95, 'temperature': 0.9, 'repetition_penalty': 1.1} -> ROUGE1=0.026 ROUGE2=0.001\n"
     ]
    }
   ],
   "source": [
    "val_results = []\n",
    "for cfg in decode_grid:\n",
    "    scores = eval_rouge_model_batched(val_prefixes, val_refs, cfg, bs=16, max_new_tokens=64)\n",
    "    val_results.append((cfg, scores))\n",
    "    print(\"CFG:\", cfg, \"-> ROUGE1={:.3f} ROUGE2={:.3f}\".format(scores[\"rouge1\"], scores[\"rouge2\"]))\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3e31de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best DistilGPT2 VAL: {'do_sample': True, 'top_k': 0, 'top_p': 0.9, 'temperature': 0.7} {'rouge1': 0.029377097270387, 'rouge2': 0.001326424644015844}\n"
     ]
    }
   ],
   "source": [
    "best_cfg, best_scores = sorted(val_results, key=lambda x: x[1][\"rouge2\"], reverse=True)[0]\n",
    "print(\"Best DistilGPT2 VAL:\", best_cfg, best_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22cd04c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SAMPLE VAL COMPLETIONS (DistilGPT2):\n",
      "\n",
      "--- Example 788 ---\n",
      "INPUT (X):     gaylib1986 it was just areply on you facebook status that you\n",
      "PREDICTED:   can now delete your posts without the permission of any third party.\n",
      "I would have had to go back and forth with them, because I know they could do anything at all in this way so if anyone ever gets a violation from someone else (like me) please contact us directly or by email - we'll take whatever\n",
      "TARGET:  were a little upset\n",
      "\n",
      "--- Example 861 ---\n",
      "INPUT (X):     setuid oh i learned my lesson from seinfeld was\n",
      "PREDICTED:   an excellent example of how to get better at it.\n",
      "In the future I'll be working on a post about Seinfeld's interaction with John Gillis (another man who has been teaching me for years) that will help us understand what is really happening in our lives and show you why we are here, along with\n",
      "TARGET:  tempted but didnt\n",
      "\n",
      "--- Example 82 ---\n",
      "INPUT (X):     ashmrx\n",
      "PREDICTED:  vwCqYXU9RQdVkG7Zn3t6LxJ1bP4Wg+cBj2z8yMpOGaSlv5eNl33abfNTK0r26H35unobIoFhO\n",
      "TARGET:  sadlyyes\n",
      "\n",
      "--- Example 530 ---\n",
      "INPUT (X):     sleep over at joshs house because he insists on us watching i am legend work\n",
      "PREDICTED:   and it is to be said that we are all in love.\n",
      "I have a huge number of boyfriends now, many from London who live there with me (and they can‍t stop talking). I want to go back here every time if you like what the hell this guy has done for my life...\n",
      "TARGET:  at 7 yuck more money yay\n",
      "\n",
      "--- Example 1047 ---\n",
      "INPUT (X):     fabeku that sounds like an awesome day have\n",
      "PREDICTED:   come to the rescue of a friend who's been kidnapped by her own husband.\n",
      "The pair were rescued and reunited on January 6th, 2016 after two days in captivity with their estranged wife\n",
      "TARGET:  a terrific time\n"
     ]
    }
   ],
   "source": [
    "def show_examples(prefixes, references, generator_fn, n=5, seed=0):\n",
    "    rnd = random.Random(seed)\n",
    "    idxs = rnd.sample(range(min(len(prefixes), len(references))), k=min(n, len(prefixes)))\n",
    "    for i in idxs:\n",
    "        pred = generator_fn([prefixes[i]])[0]\n",
    "        print(f\"\\n--- Example {i} ---\")\n",
    "        print(\"INPUT (X):    \", prefixes[i])\n",
    "        print(\"PREDICTED: \", pred)\n",
    "        print(\"TARGET: \", references[i])\n",
    "\n",
    "print(\"\\nSAMPLE VAL COMPLETIONS (DistilGPT2):\")\n",
    "show_examples(val_prefixes, val_refs,\n",
    "              lambda p: complete_with_distilgpt2_batched(p, max_new_tokens=64, **best_cfg), n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea03e18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
