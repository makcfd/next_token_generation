{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "843f601e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maksim.fediushkin/PyPetProjects/yandex_nlp/final_project_sprint_2/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gc, random\n",
    "from typing import List, Tuple\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from rouge_score import rouge_scorer\n",
    "from src.data_utils import clean_string, split_x_target_by_words\n",
    "from src.lstm_model import load_lstm_lm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f6e352d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train texts: 1280000, Val_Test texts: 320000\n",
      "Val texts: 160000, Test texts: 160000\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./data/tweets_cleaned.csv\")\n",
    "val_test_size = 0.20\n",
    "test_size = 0.50\n",
    "\n",
    "train_texts, val_test_texts = train_test_split(list(df[\"cleaned_text\"]), test_size=val_test_size, random_state=42)\n",
    "print(f\"Train texts: {len(train_texts)}, Val_Test texts: {len(val_test_texts)}\")\n",
    "val_texts, test_texts = train_test_split(val_test_texts, test_size=test_size, random_state=42)\n",
    "print(f\"Val texts: {len(val_texts)}, Test texts: {len(test_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb957dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: mps\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    torch.device(\"cuda\") if torch.cuda.is_available() else\n",
    "    torch.device(\"mps\")  if torch.backends.mps.is_available() else\n",
    "    torch.device(\"cpu\")\n",
    ")\n",
    "print(\"device:\", device)\n",
    "\n",
    "\n",
    "def build_eval_pairs(texts, max_examples: int = 1500):\n",
    "    pairs = []\n",
    "    for t in texts:\n",
    "        t = clean_string(t)\n",
    "        if not t:\n",
    "            continue\n",
    "        x, y = split_x_target_by_words(t)\n",
    "        if x and y:\n",
    "            pairs.append((x, y))\n",
    "        if len(pairs) >= max_examples:\n",
    "            break\n",
    "    X = [p for p,_ in pairs]\n",
    "    Y = [r for _,r in pairs]\n",
    "    print(f\"Built {len(X)} prefix/target pairs.\")\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec9744fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def generate_lstm(prefixes: List[str], tokenizer, model, pad_id, eos_id, device, max_new_tokens=64):\n",
    "    ids = [torch.tensor(tokenizer.encode(p, add_special_tokens=False), dtype=torch.long) for p in prefixes]\n",
    "    if not ids:\n",
    "        return []\n",
    "    batch = pad_sequence(ids, batch_first=True, padding_value=pad_id).to(device)\n",
    "    out_ids = model.generate(batch, max_new_tokens=max_new_tokens, eos_id=eos_id)\n",
    "    start = batch.size(1)  # right padded\n",
    "    return [tokenizer.decode(out_ids[i, start:].tolist(), skip_special_tokens=True)\n",
    "            for i in range(out_ids.size(0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05a67981",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gpt_name = \"distilbert/distilgpt2\"\n",
    "gpt_tok = AutoTokenizer.from_pretrained(gpt_name, padding_side=\"left\")  # left padding for decoder\n",
    "if gpt_tok.pad_token_id is None:\n",
    "    gpt_tok.pad_token = gpt_tok.eos_token\n",
    "gpt = AutoModelForCausalLM.from_pretrained(\n",
    "    gpt_name,\n",
    "    torch_dtype=(torch.float16 if device.type in {\"cuda\",\"mps\"} else None),\n",
    "    low_cpu_mem_usage=True\n",
    ").to(device).eval()\n",
    "gpt.config.pad_token_id = gpt_tok.pad_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2563d23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.inference_mode()\n",
    "def generate_gpt2(prefixes: List[str], max_new_tokens=64, do_sample=True, top_k=50, top_p=0.95, temperature=0.8, repetition_penalty=1.1):\n",
    "    enc = gpt_tok(prefixes, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    input_ids = enc[\"input_ids\"].to(device)\n",
    "    attention_mask = enc[\"attention_mask\"].to(device)\n",
    "    kwargs = dict(\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        eos_token_id=gpt_tok.eos_token_id,\n",
    "        pad_token_id=gpt_tok.pad_token_id,\n",
    "        use_cache=True,\n",
    "        do_sample=do_sample,\n",
    "        top_k=top_k, \n",
    "        top_p=top_p, \n",
    "        temperature=temperature, \n",
    "        repetition_penalty=repetition_penalty\n",
    "    )\n",
    "    out = gpt.generate(input_ids=input_ids, attention_mask=attention_mask, **kwargs)\n",
    "    lens = attention_mask.sum(1)  # left padded\n",
    "    return [gpt_tok.decode(out[i, int(L.item()):], skip_special_tokens=True) for i, L in enumerate(lens)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c54f71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = rouge_scorer.RougeScorer([\"rouge1\",\"rouge2\"], use_stemmer=False)\n",
    "\n",
    "def eval_rouge_batched(prefixes, references, gen_fn, bs=16):\n",
    "    r1 = r2 = n = 0\n",
    "    for s in range(0, len(prefixes), bs):\n",
    "        preds = gen_fn(prefixes[s:s+bs])\n",
    "        for p, g in zip(preds, references[s:s+bs]):\n",
    "            sc = scorer.score(g, p)\n",
    "            r1 += sc[\"rouge1\"].fmeasure\n",
    "            r2 += sc[\"rouge2\"].fmeasure\n",
    "            n  += 1\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    return {\"rouge1\": r1/max(n,1), \"rouge2\": r2/max(n,1)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37a2c707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built 1500 prefix/target pairs.\n"
     ]
    }
   ],
   "source": [
    "test_prefixes, test_refs = build_eval_pairs(test_texts, max_examples=1500)\n",
    "\n",
    "run_dir = \"models/lstm_lm-20250912-181525\" \n",
    "tok_lstm, lstm_model, pad_id, eos_id = load_lstm_lm(run_dir, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0be70079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TEST COMPARISON (3/4 → 1/4, word-based) ===\n",
      "LSTM  : ROUGE1=0.043 ROUGE2=0.001\n",
      "GPT-2 : ROUGE1=0.028 ROUGE2=0.001\n",
      "\n",
      "— Example 163 —\n",
      "PREFIX   : charlimon haha your mint come on msn if you can\n",
      "TARGET   : we need a natter\n",
      "LSTM     : ##t\n",
      "DistilG2 :  afford it for a little bit and be honest, i am so very excited to see my new car.\n",
      "This is the first example of how I will produce something that looks great in comparison with anything from their classic Mercedes E-Class model or just like any other brand which has some kind of factory build quality but lacks\n",
      "\n",
      "— Example 1160 —\n",
      "PREFIX   : zarigee i got mcdonalds surpisingly its not a popular drunk food option its sucks that\n",
      "TARGET   : all diners arent 24hrs here anymore\n",
      "LSTM     : ##s the best\n",
      "DistilG2 :  it just wont be the first place in town. if you want to get some extra beer, no problem with this and dont even have an entire bar at home because there are only 10 people who can drink them..\n",
      "\n",
      "Also what about all of these places? What do I mean by those things ?\n",
      "\n",
      "— Example 1153 —\n",
      "PREFIX   : daisywalking what that is still really good\n",
      "TARGET   : only one c\n",
      "LSTM     : \n",
      "DistilG2 : ,›\n",
      "And to do it right.\n",
      "\n",
      "— Example 463 —\n",
      "PREFIX   : 2hrs sleep bad day\n",
      "TARGET   : at work\n",
      "LSTM     : \n",
      "DistilG2 : , but he's so busy.\n",
      "I get bored with what I'm doing and not even seeing my wife go home to the kitchen or eating all of it because she hasn't done a job for me yet. So why are we having sex? We're feeling like our kids can do nothing except eat some food instead\n",
      "\n",
      "— Example 371 —\n",
      "PREFIX   : whoooos coming with me to pub quiz tonight almost everyone else\n",
      "TARGET   : is out of town\n",
      "LSTM     : \n",
      "DistilG2 :  I know will be talking about my life, and as you might have seen at the time of writing this article.\n",
      "I'm sorry we couldn't get that wrong from us here in London... but what are your thoughts on going forward? Have fun watching some live action videos below!\n"
     ]
    }
   ],
   "source": [
    "lstm_gen = lambda batch: generate_lstm(batch, tokenizer=tok_lstm, model=lstm_model,\n",
    "                                       pad_id=pad_id, eos_id=eos_id, device=device, max_new_tokens=64)\n",
    "best_cfg = dict(do_sample=True, top_k=50, top_p=0.95, temperature=0.8, repetition_penalty=1.1)\n",
    "gpt_gen  = lambda batch: generate_gpt2(batch, max_new_tokens=64, **best_cfg)\n",
    "\n",
    "print(\"\\n=== TEST COMPARISON (3/4 → 1/4, word-based) ===\")\n",
    "lstm_scores = eval_rouge_batched(test_prefixes, test_refs, lstm_gen, bs=16)\n",
    "print(\"LSTM  : ROUGE1={:.3f} ROUGE2={:.3f}\".format(lstm_scores[\"rouge1\"], lstm_scores[\"rouge2\"]))\n",
    "\n",
    "gpt_scores  = eval_rouge_batched(test_prefixes, test_refs, gpt_gen, bs=16)\n",
    "print(\"GPT-2 : ROUGE1={:.3f} ROUGE2={:.3f}\".format(gpt_scores[\"rouge1\"], gpt_scores[\"rouge2\"]))\n",
    "\n",
    "\n",
    "for i in random.sample(range(len(test_prefixes)), k=min(5, len(test_prefixes))):\n",
    "    lp = lstm_gen([test_prefixes[i]])[0]\n",
    "    gp = gpt_gen([test_prefixes[i]])[0]\n",
    "    print(f\"\\n— Example {i} —\")\n",
    "    print(\"PREFIX   :\", test_prefixes[i])\n",
    "    print(\"TARGET   :\", test_refs[i])\n",
    "    print(\"LSTM     :\", lp)\n",
    "    print(\"DistilG2 :\", gp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e8d33a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
